{
  "id": "jupyter-lab-llama-ls6-dev",
  "version": "0.0.8",
  "description": "<h2>Motivation</h2>\n<p>The Sites and Stories cookbooks are to provide an environment to run Large Language Models (LLM) on TACC systems. Specifically, this cookbook shows how to use LLM's to generate a Retrieval-Augmented Generation (RAG) query a personalized pdf library as a corpus. </p>\n<p>This environment uses the <a href=\"https://www.llamaindex.ai/\">LLAMA Index</a> python library to organize and query the corpus, and downloads/uses <a href=\"https://huggingface.co/models?other=text-generation-inference\">Hugging Face</a> models for the queries. </p>\n<h2>Authors</h2>\n<p>William Mobley - wmobley@tacc.utexas.edu</p>\n<h2>Contributors</h2>\n<p><a href=\"https://github.com/In-For-Disaster-Analytics/sites-and-stories-nlp/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=In-For-Disaster-Analytics/sites-and-stories-nlp\" />\n</a></p>\n<h2>Structure</h2>\n<h2>Foundations</h2>\n<p>There are currently two files to run the RAG model. The <a href=\"LLamaIndex.ipynb\">LLamaIndex.ipynb</a> file streamlines access to the LLMs; loads the requisite corpus and allows the user to query the pdf's a retrieves answers with citations. </p>\n<p>The <a href=\"LLM_location.py\">LLM_location.py</a> file provides the framework to run the notebook. In this file you can add other LLM Models and provide further customization. </p>\n<p>Alternatively you can use pieces within the LLM_location file to develop your own system. </p>\n<h2>Running the Notebooks</h2>",
  "owner": "${apiUserId}",
  "enabled": true,
  "runtime": "SINGULARITY",
  "runtimeVersion": null,
  "runtimeOptions": ["SINGULARITY_RUN"],
  "containerImage": "docker://ghcr.io/in-for-disaster-analytics/tap_llmrepository-docker:0.0.7",
  "jobType": "BATCH",
  "maxJobs": -1,
  "maxJobsPerUser": -1,
  "strictFileInputs": true,
  "jobAttributes": {
    "description": null,
    "dynamicExecSystem": false,
    "execSystemConstraints": null,
    "execSystemId": "ls6",
    "execSystemExecDir": "${JobWorkingDir}",
    "execSystemInputDir": "${JobWorkingDir}",
    "execSystemOutputDir": "${JobWorkingDir}/output",
    "execSystemLogicalQueue": "gpu-a100",
    "archiveSystemId": "cloud.data",
    "archiveSystemDir": "HOST_EVAL($HOME)/tapis-jobs-archive/${JobCreateDate}/${JobName}-${JobUUID}",
    "archiveOnAppError": true,
    "isMpi": false,
    "mpiCmd": null,
    "cmdPrefix": "mkdir $PWD/work $PWD/home $PWD/scratch;",
    "parameterSet": {
      "appArgs": [
        {
          "name": "Update cookbook",
          "description": "Control whether the system will update the existing cookbook with the latest version available. This option is irrelevant if you are running the cookbook for the first time.",
          "inputMode": "REQUIRED",
          "arg": "false",
          "notes": {
            "enum_values": [
              {
                "true": "Update to latest version"
              },
              {
                "false": "Keep current version"
              }
            ]
          }
        },
        {
          "name": "Update conda environment",
          "description": "Control whether the system will update the existing conda environment with the latest version available. This option is irrelevant if you are running the cookbook for the first time.",
          "inputMode": "REQUIRED",
          "arg": "false",
          "notes": {
            "enum_values": [
              {
                "true": "Update to latest version."
              },
              {
                "false": "Keep current version"
              }
            ]
          }
        },
        {
          "name": "Git repository url",
          "description": "The URL of the git repository to clone.",
          "inputMode": "REQUIRED",
          "arg": "https://github.com/In-For-Disaster-Analytics/sites-and-stories-nlp.git",
          "notes": {
            "isHidden": true
          }
        },
        {
          "name": "Git branch",
          "description": "The branch of the git repository to clone.",
          "inputMode": "REQUIRED",
          "arg": "dev",
          "notes": {
            "isHidden": false,
            "enum_values": [
              {
                "jupyterenv": "Jupyter Lab environment"
              },
              {
                "dev": "Development"
              },
              {
                "main": "main"
              }
            ]
          }
        }
      ],
      "containerArgs": [
        {
          "name": "Jupyter Mounts",
          "description": "Mount for TAP functions and user dirs",
          "inputMode": "FIXED",
          "arg": "--bind /share,$WORK:$PWD/work,$HOME:$PWD/home,$SCRATCH:$PWD/scratch",
          "notes": {
            "isHidden": true
          }
        },
        {
          "name": "NVIDIA Flag",
          "description": "Flag to enable NVIDIA cuda",
          "inputMode": "FIXED",
          "arg": "--nv",
          "notes": {
            "isHidden": true
          }
        }
      ],
      "schedulerOptions": [
        {
          "name": "TACC Scheduler Profile",
          "description": "Scheduler profile for HPC clusters at TACC",
          "inputMode": "FIXED",
          "arg": "--tapis-profile tacc-apptainer",
          "notes": {
            "isHidden": true
          }
        },
        {
          "name": "TAP Session Substring",
          "description": "TAP Functions require the substring 'tap_' and in the slurm job name in order to function.",
          "inputMode": "FIXED",
          "arg": "--job-name ${JobName}-tap_",
          "notes": {
            "isHidden": true
          }
        }
      ],
      "envVariables": [
        {
          "key": "APPTAINER_PWD",
          "value": "/home/jovyan/work",
          "description": "Start terminals in the working directory where host volumes are mounted.",
          "inputMode": "FIXED",
          "notes": {
            "isHidden": true
          }
        }
      ],
      "archiveFilter": {
        "includes": [],
        "excludes": [],
        "includeLaunchFiles": true
      }
    },
    "fileInputs": [],
    "fileInputArrays": [],
    "nodeCount": 1,
    "coresPerNode": 16,
    "memoryMB": 128000,
    "maxMinutes": 120,
    "subscriptions": [],
    "tags": []
  },
  "tags": ["portalName: ALL"],
  "notes": {
    "label": "LLama Index Environment (LoneStar 6) Development",
    "helpUrl": "https://github.com/In-For-Disaster-Analytics/sites-and-stories-nlp/tree/jupyterenv",
    "html": "This app is a Jupyter Lab environment that allows you to use LLama to index data on the TACC Cloud Data Archive. It is a work in progress and is not yet fully functional.",
    "hideNodeCountAndCoresPerNode": true,
    "isInteractive": true,
    "icon": "jupyter",
    "category": "Data Processing",
    "queueFilter": ["gpu-a100", "gpu-a100-dev", "development"]
  }
}
